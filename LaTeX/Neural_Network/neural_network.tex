%% sudo yum install tetex
%% sudo yum install texlive-elsarticle.noarch  texlive-sttools.noarch texlive-lipsum.noarch
%% pdflatex neural_network.tex && bibtex neural_network.aux && pdflatex neural_network.tex && pdflatex neural_network.tex
%% can try to setup with 
%% yum -y install 'tex(multirow.sty)'


%\documentclass[a4paper,12pt]{}
\documentclass[final, paper=letter,5p,times,twocolumn]{elsarticle}
%\documentclass[preprint,review,8pt,times]{elsarticle}


%% or use the graphicx package for more complicated commands
%\usepackage{changebar}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm,multicol,bm,lipsum} % Math packages
\usepackage{cuted}
%\usepackage{dsfont} % mathds{1}
%\usepackage{widetext} % 
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
%
%\usepackage[]{algorithm2e}
%% Macro
\newcommand{\ToDo}[1]{ToDo: \textbf{\textit{#1}}}
\newcommand{\CA}{computational anatomy}
%
\newdefinition{definition}{Definition}%
\newtheorem{theorem}{Theorem}%
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
%\newproposition{proposition}{Proposition}%
%\newlemma{lemma}{Lemma}%
%\AtEndEnvironment{theorem}{\null\hfill\qedsymbol}%

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frontmatter}

\title{Neural network}

\author[label1]{Yann Cobigo\corref{cor1}}
\address[label1]{University of California, San Francisco | ucsf.edu}
%\address[label2]{Address Two\fnref{label4}}

%\cortext[cor1]{I am corresponding author}
%\fntext[label3]{I also want to inform about\ldots}
%\fntext[label4]{Small city}

\ead{yann.cobigo@ucsf.edu}
\ead[url]{https://github.com/YannCobigo}

%% \author[label5]{Author Two}
%% \address[label5]{Some University}
%% \ead{author.two@mail.com}
%% 
%% \author[label1,label5]{Author Three}
%% \ead{author.three@mail.com}

\begin{abstract}
 \lipsum[11-15]
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Fijee \sep electrode \sep PEM \sep CEM
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lipsum[100-104]

\section{Multilayer perceptron}

We are using the perceptron as an explenatory mecanism introducing the local elements of a deep learning principals. The perceptron can be seen as the firing elements of the neural network, receiving information from sevaral inputs and firing when the amount of stimuli exceed a threshold. The weigths of the perceptron can be optimized using the gradient descent method. This section introduce the gradient descent method and applying different gradient strategies the perceptron fine tunes its parameters.

\subsection{Perceptrons: the building block}
In the litterature, the neural network is often refered as {\it multilayer perceptron}. In other words, the neural network is based on layers of "perceptrons". The perceptron was one of the first machine learning algorithm used with computers. It is a supervise algorithm. The principal of classification using perceptron seat on fitting linear model $X^{T}\omega > \theta$, where $\omega$ is a vector of $N$ weights, $X$ is a vector of $N$ predictors and $\theta$ is a threshold. In its generalized linear model writing it becomes the non-linear step function: $f(X) = \phi(X)^{T}\omega \in \{-1,+1\}$, where $\phi$ can be non-linear. $\phi = (1, \phi_{1}, \cdots, \phi_{N})^{T}$ is a vector of $N$ components plus a $1$ which is counter part to the bias corresponding to the threshold. The weights $\omega = (\omega_{0} = \theta, \omega_{1}, \cdots, \omega_{N})^{T}$ is a vector with $N$ weights plus $\omega_{0} = \theta$ associated to the bias. It is convinient to move the unknown in the same vector and write the weights and the bias as $\omega = (\omega_{0}, \omega_{1}, \cdots, \omega_{N})^{T}$, then the predictors become $\phi = (\phi_{0} = 1, \phi_{1}, \cdots, \phi_{N})^{T}$. \\
The perceptron is a binary classification method, the response of the training model is $t \in \{-1,+1\}$. Observing that {\it perceptron criteria} $t \times \phi(X)^{T}\omega > 0$ is always positive, the minimization is based on minimizing the number miss-classified elements: \\

\begin{equation}
E = - \sum_{c \in \mathcal{M}} t_{n} \times \phi(X_{n})^{T}\omega
\label{eq:Optimization_perceptron}
\end{equation}

The set $\mathcal{M}$ gather all the miss-classified elements. The minimum is reached using the gradient descent algorithm $\omega^{e+1} = \omega^{e} - \eta \bm{\nabla} E$, where $e$ is the iterative epoque, $\eta$ is the learning rate, and:

$$
\bm{\nabla} E = - \sum_{c \in \mathcal{M}} t_{n} \times \phi(X_{n})
$$

A semantic clarification needs to be done about the multi-layer perceptron. As mentioned, $f$ is a discontinued step function, whereas neural network use step continuous, or sigmoidal, function like hyperbolic tangent, logistic, \dots In those terms, a neural network is not exactly a multilayer perceptron, but close enough.

\subsection{Gradient descent cement between blocks}

The gradient descent is an iterative algorithm that tries to reach a local minimum in the optimization process. It is based on the link between the total differential of a scalar function and the gradient:

$$
dE = \bm{\nabla}E \cdot \bm{d \omega}
$$

Selecting $\bm{d\omega} = -\eta \bm{\nabla}E$, where $\eta$ is a positive parameter called the learning rate. The total differential of the cost functuion $E$ can be written: $dE = -\eta \bm{\nabla}E \cdot \bm{\nabla}E < 0$, meaning for each iteration $e$, $\omega^{e+1} = \omega^{e} - \eta \bm{\nabla} E$, the cost function is assured to decrease. 
Faster results were obtained by correcting the weights after each miss-classification, {\it stochastic gradient descent}, or after small batch of miss-classified elements, {\it mini-batch gradient descent}. The same principals are applied to the neural network training.\\

\subsubsection{Hessian optimization}

The total differential of the cost function can be pushed an order higher:

$$
dE = (\bm{\nabla}E)^{T} \bm{d \omega} + (\bm{d \omega})^{T}H\bm{d \omega} + \dots
$$

Minimizing the right hand of the total differential, we have $\bm{d \omega} = - H^{-1} \bm{\nabla}E$. Theorically, the Hessian optimization reach the minimum of the cost function fast, because the method holds information about the gradient and how it changes. However, the iteration is controled using smaller steps: $\omega^{e+1} = \omega^{e} - \eta  H^{-1} \bm{\nabla}E$, where $\eta$ is a positive parameter known as the learning rate. Beyond the interesting properties offered by the Hessian optimization, it is costly to use this method. We consider a problem with $n$ weights and biases, the gradient will be calculated over the $n$ weights and biases and the Hessian matrix will be calculated over $n \times n$ weights, then inversed.

\subsubsection{Momentum based gradient descent}
[Momentum based gradient descent]

\subsection{Forward propagation, or feed-forward algorithm}
Instead of relying on one perceptron, we use a layer of $L_{1}$ perceptrons and a bias. Each perceptron of the layer $(l_{1})$ loads all the inputs of the layer $(l_{0})$. The activation $l_{1}$ of the layer $(l_{1})$ is the dot product $a_{l_{1}} = \sum_{(l_{0})} \omega_{l_{1}l_{0}}z_{l_{0}}$ where the indexes of the layer $(l_{0})$ are $l_{0} = 0, \dots, L_{0}$, of the layer $(l_{1})$ are $l_{1} = 1, \dots, L_{1}$, and $z_{l_{0} = 0} = 1$ is the predictor conter part of the bias and $\omega_{l_{1}l_{0} = 0} = \omega_{l_{1}0}$ is the bias. At each layer, activations can be represented as a vector:

\begin{eqnarray*}
  \left(
  \begin{array}{c}
    a_{l_{1}} \\
    \vdots \\
    a_{L_{1}}
  \end{array}
  \right) = \left(
  \begin{array}{cccc}
    \omega_{10} & \omega_{11} & \cdots & \omega_{1L_{0}} \\
    \vdots     &            & \cdots & \vdots \\
    \omega_{L_{1}0} & \omega_{L_{1}1} & \cdots & \omega_{L_{1}L_{0}} \\
  \end{array}
  \right)  \left(
  \begin{array}{c}
    z_{l_{0} = 0} = 1 \\
    z_{l_{0} = 1} \\
    \vdots \\
    z_{L_{0}} 
  \end{array}
  \right)
\end{eqnarray*}

Which can be writen in a compact manner: $\bm{a}^{(l_{1})} = \Omega_{10}\bm{z}^{(l_{0})}$, and the matrix $\Omega_{10}$ is a $L_{1}\times(L_{0}+1)$ weights matrix. The activation function $\bm{z}^{(l_{1})} = (z_{l_{1} = 0} = 1, f(\bm{a}^{(l_{1})}))^{T}$ where $f$ is a smooth non-linear step function. If we have a second inner layer, the total number of weights will be $L_{1}\times(L_{0}+1) + L_{2}\times(L_{1}+1)$. Then for $n$ inner layers, we will have $L = \sum_{i = 1}^{n}L_{i}\times(L_{i-1}+1)$ weights. In this document $(l_{0})$ and $(l_{k})$ will represents, respectively, the input and output layers.

\subsection{Backward propagation of the errors}

The cost function is estimated at the last layer of the neural network. In purpose to adjust the weights in the inner layers we use the backward propagation method to propagate the error through all the layers. Let $(l_{u})$ be some layer in the set of inner layers:

\begin{equation*}
  \frac{\partial E_{i}}{\partial \omega_{l_{u}l_{v}}} = \frac{\partial E_{i}}{\partial a_{l_{u}}} \frac{\partial a_{l_{u}}}{\partial \omega_{l_{u}l_{v}}} = \delta_{l_{u}}\frac{\partial a_{l_{u}}}{\partial \omega_{l_{u}l_{v}}} 
\end{equation*}

We define $\delta_{l_{u}}$ the error at the neuron $l_{u}$ on the layer $(l_{u})$. The cost function $E_{i}$ represents the error on the input $i$. The element $l_{v}$ represents some element on the layer $(l_{v})$. We are going to work with consecutive layers and, in general, $(l_{v}) = (l_{u-1})$. At the last layer $(l_{k})$, the gradient components are estimated using:

\begin{equation}
  \frac{\partial E_{i}}{\partial \omega_{l_{k}l_{k-1}}} = \delta_{l_{k}}\frac{\partial a_{l_{k}}}{\partial \omega_{l_{k}l_{k-1}}} 
\label{eq:backprob_layer_k}
\end{equation}

For the subsequent layers, we decompose the partial derivative of the cost function, using the chain rule, to make appear the link between the layer $(l_{u})$ and the layer $(l_{u+1})$. 

\begin{equation}
  \begin{split}
    \frac{\partial E_{i}}{\partial \omega_{l_{u}l_{u-1}}} = &  \frac{\partial E_{i}}{\partial a_{l_{u}}} \frac{\partial a_{l_{u}}}{\partial \omega_{l_{u}l_{u-1}}} = \delta_{l_{u}} \frac{\partial a_{l_{u}}}{\partial \omega_{l_{u}l_{u-1}}}  \\
    = & \sum_{l'_{u+1}}\frac{\partial E_{i}}{\partial a_{l'_{u+1}}} \frac{\partial a_{l'_{u+1}}}{\partial a_{l_{u}}} \frac{\partial a_{l_{u}}}{\partial \omega_{l_{u}l_{u-1}}}   \\
    = & \sum_{l'_{u+1}} \delta_{l'_{u+1}} \frac{\partial a_{l'_{u+1}}}{\partial a_{l_{u}}} z_{l_{u-1}}   \\
    = & \sum_{l'_{u+1}} \delta_{l'_{u+1}} \sum_{l'_{u}} \omega_{l'_{u+1}l'_{u}} f'(a_{l'_{u}}) \delta_{l'_{u}l_{u}} z_{l_{u-1}}   \\
    = & \sum_{l'_{u+1}} \delta_{l'_{u+1}} \omega_{l'_{u+1}l_{u}} f'(a_{l_{u}}) z_{l_{u-1}}   \\
  \end{split}
\label{eq:backprob}
\end{equation}

Equations~(\ref{eq:backprob_layer_k}) and~(\ref{eq:backprob}) are the back propagation equations. In the followin section, we are going to apply the back propagation equations to different cost functions.


\subsection{Inputs}
For the different type of neural networks we would like to accomplish, the input classes must be very different. Most of the time we are going to work with medical images, implying using a dimensional correlation like the convolutional neural network (CNN). Unlike most of the machine learning algorithm taking vectors as input, CNN void the space decorrelation from the vectorization. However, providing different classes for different inputs could offer some flexibility. The input could also be a sampling of aphasia speech to discriminate with other groups.

\ToDo{check all sort of input we would want.} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cost functions and activation functions}

A neuron is represented with an activation $a_{l_{i}} = \omega_{l_{i}l_{j}} z_{l_{j}}$, integrating pulses from activation function $z_{l_{j}}$ of the previouse layer, $(l_{j})$. The new activation function $z_{l_{i}} = f(a_{l_{i}})$ is the firering process of the neuron $l_{i}$. Usually, the activation function is a non-linear continuous step function. 

\subsection{Activation functions}

Activation functions used for layers of neurons are the hyperbolic tangent, the logistic or the rectified linear unit (ReLU). Except for special layers like the last layer of a fully connected network. The derivative of these functions are:

\begin{itemize}
\item hyperbolic tangent: $f = \tanh \rightarrow f' = 1 - \tanh \times \tanh$,
\item logistic: $\sigma = \frac{1}{1+e^{-x}} \rightarrow \sigma' = \sigma (1 - \sigma)$,
\item ReLU: $x^{+} = \max(0,x) \rightarrow 1$ if $x^{+} = x$, 0 otherwise.
\item Unity: $f(x) = x \rightarrow f' = 1$
\end{itemize}
\ToDo{how to make the chose of the activation function.}

\subsection{Cost functions}
\label{sec:cost_functions}

The lastest layer of a neural network has its activation function linked to the cost function. Pairing the cost function with the activation function allow better optimization. Fitting a response would be best using the least-square cost function. In that case any activation function would work out. Testing binary classification would use a Bernouilli cost function. So on and so forth.\\
The back propagation algorithm is going to make use of the gradient descent algorithm, and propagating the error through the different layers of the neural network is going to make use of the differentiability of the cost function in $a_{l_{i}}^{(l_{i})}$, {\it i.e.} the activation $a_{l_{i}}$ at the layer $(l_{i})$: {\it error} $\delta_{i}^{(l_{i})} = \frac{\partial E}{\partial a_{l_{i}}}$.

\subsubsection{Least-square cost function}

The least-square methode is the most general cost function and could be applyed along a large spectrum of problems. A simple example would be fitting a courve, or matching images. 

\begin{equation}
  E = \frac{1}{2} \sum_{i = 1}^{n} (\bm{z}(\bm{x}_{i}) - \bm{t}_{i})^{T}(\bm{z}(\bm{x}_{i}) - \bm{t}_{i})
  \label{eq:least_squarre}
\end{equation}

The convexity of the cost function make it appealing in the gradient descent algorithm. The calculation of the error at the last layer, in the case of the least square, is:

\begin{equation}
  \delta_{k}^{(l_{k})} = \frac{\partial E}{\partial a_{l_{k}}} = \sum_{i = 1}^{n} (\bm{z}(\bm{x}_{i}) - \bm{t}_{i})\frac{\partial \bm{z}(\bm{x}_{i})}{\partial a_{l_{k}}}
  \label{eq:least_squarre_error}
\end{equation}
 
If the activation function is the unit function, the propagation throught the first layer is simply $(\bm{z}(\bm{x}_{i}) - \bm{t}_{i})$.
\subsubsection{Binary classification}

segmentation?

\subsubsection{Multiclass classification}
\label{sec:MulticlassClassification}

For the multiclass classification with $K$ classes the output is a vector. The sum of all dimensions is the unity. Each dimension of the output vector is the probability of the class $k$ among $K$ classes: $y_{x} = \exp(a_{k}) / \mathcal{Z}$. The denominator is the partition function normalizint the probability distribution, and $a_{k}$ is the activation. This activation function is called the soft-maximum. We use the mixture trick to determine the output probability. If we have $N$ samples, then the probability to get the element $\bm{t}$ is:

\begin{equation}
p(\bm{t} | \bm{x}_{n}, \bm{w}) = \prod_{n=1}^{N}\prod_{k=1}^{K} y_{k}(\bm{x}_{n})^{t_{k}}
\end{equation}

Where $t_{k} \in \{0,1\}$ is one-of-K element. the other elements $k' \ne k$ are zero. The cost function is the negative logarithm which is:

\begin{equation}
E = - \sum_{n=1}^{N}\sum_{k=1}^{K} t_{k} \ln (y_{k}(\bm{x}_{n}))
\end{equation}

Is has been shown good optimization results were reached by using the soft maximum as activation function in the case of multiclass classification.

The soft maximum is often used in classification problem involving multiple output classes. the output can be interpreted as probability following the distribution:

\begin{equation}
  g(a_{l_{k}}) = \frac{e^{a_{l_{k}}}}{\mathcal{Z}}
  \label{soft_max}
\end{equation}

Where $\mathcal{Z}$ is the partition function normalizing the distribution. Derivation the soft maximum in function of the parameter $a_{k}$ gives us:

\begin{equation*}
  \begin{split}
    \frac{\partial g(a_{k})}{\partial a_{k'}} = & \left \lbrack \delta_{kk'} e^{a_{k'}} \mathcal{Z} - e^{a_{k'}}\frac{\partial \mathcal{Z}}{\partial a_{k'}} \right \rbrack \times \frac{1}{\mathcal{Z}^{2}}\\
    = & \left \lbrack \delta_{kk'} e^{a_{k'}} \mathcal{Z} - e^{a_{k'}} \sum_{k''} \delta_{kk''} e^{a_{k''}}  \right \rbrack \times \frac{1}{\mathcal{Z}^{2}}\\
    = & \frac{\delta_{kk'} e^{a_{k'}}}{\mathcal{Z}} - z_{k}z_{k'}\\
  \end{split}
\end{equation*}



\subsubsection{Least-squarre cost function}

the least-squarre cost function is a widly used cost function in machine learning. large part of our algorithm use this cost function to minimize in the gradient descent. We have $n$ participents and $O$ type of outputs. The cost function represent the difference from the expected output $t_{i}$ and the calculated output $z_{i}$.

\begin{equation}
  E = \frac{1}{2n} \sum_{i = 1}^{n} \sum_{o' = 1}^{(O)} \| z^{(o')} - t^{(o')} \|^{2}
  \label{least_squarre}
\end{equation}

The minimization of the cost function, using the local error, gives us:

\begin{equation}
  \begin{split}
    \frac{\partial E}{\partial a_{x,lu+1}^{(o)}} = & \frac{1}{n} \sum_{i = 1}^{n} \frac{\partial z_{x,lu+1}^{(o')}}{\partial a_{x,lu+1}^{(o)}} (z^{(o')} - t^{(o')}) \delta_{oo'} \\
    = & \frac{1}{n} \sum_{i = 1}^{n}  f'(a_{x,lu+1}^{(o)}) (z^{(o)} - t^{(o)}) = \frac{1}{n} \sum_{i = 1}^{n}  E_{i}
  \end{split}
\end{equation}

If the function transforming the activation into an activation function is a linear, the local error is: $\delta^{(o)} = (z^{(o)} - t^{(o)})$.

\subsubsection{Cross entropy cost function}
\label{Cross_entropy_cost_function_sec}

In information theory, the cross entropy between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution $q$, rather than the "true" distribution $p$. In classification, the cross entropy for the distributions $t$, labels, and $z$, reconstructed solution, over a given set is defined as follows:

\begin{equation}
  E = - \sum_{i = 1}^{n}\sum_{l'_{k} = 1}^{L_{k}} t_{l'_{k}} \ln z_{l'_{k}} =  - \sum_{i = 1}^{n} E_{i}
  \label{cross_entropy}
\end{equation}

$t_{l'_{k}}$ represents the known label, $z_{l'_{k}} = y_{k}$ represents the response of the system, $n$ is the number of participants. 

In this section, we are going to derivative the cross entropy~(\ref{cross_entropy}). To simplify the equation, we are derivation the cost function only for one subject and omitted the subscript $i$. At the output layer, the activation is $a_{l_{k}} = \sum_{l_{k-1} = 0}^{L_{k-1}} \omega_{l_{k}l_{k-1}} z_{l_{k-1}}$. The derivative of the cost function in function of the activation is called the {\it error} and is written:

\begin{equation}
  \delta_{k} = \frac{\partial E_{i}}{\partial a_{l_{k}}} = - \sum_{l'_{k} = 1}^{L_{k}} t_{l'_{k}} \times \frac{1}{z_{l'_{k}}} \frac{\partial z_{l'_{k}}}{\partial a_{k}}
  \label{cost_function_error}
\end{equation}

In the case of the last activation function for the output layer is a soft maximum, we can use the result of the soft maximum derivative from the paragraph~\ref{sec:MulticlassClassification}:

\begin{equation*}
  \begin{split}
    \delta_{k} = & - \sum_{l'_{k} = 1}^{L_{k}} t_{l'_{k}} \times \frac{1}{z_{l'_{k}}} \left \lbrack  \frac{\delta_{kk'} e^{a_{l'_{k}}}}{\mathcal{Z}} - z_{l'_{k}}z_{l_{k}} \right \rbrack \\
    = & - \sum_{l'_{k} = 1}^{L_{k}} t_{l'_{k}} \times \frac{1}{z_{l'_{k}}} \frac{\delta_{kk'} e^{a_{l'_{k}}}}{\mathcal{Z}} +  \sum_{l'_{k} = 1}^{L_{k}} t_{l'_{k}} z_{l_{k}} \\
    = & - t_{l_{k}} +  z_{k} \\
  \end{split}
\end{equation*}



Which is exactly the same result as we would get in the case of a least-square cost function, if the activation function is a linear function. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convolutional neural network}

Convolutional neural networks (CNNs) are a biologically-inspired variation of the MLPs. Neurons in CNNs share weights unlike in MLPs where each neuron has a separate weight vector. This sharing of weights ends up reducing the overall number of trainable weights hence introducing sparsity. The output image is downsampled to reduce the number of weights for the next neural network and prevent overfitting.

\subsection{Description and notation}

CNNs consists of convolutional layers which are characterized by an input map $Im$, a bank of filters $Ker$ with biases $b$ per layer. Utilizing the weights sharing strategy, each neuron in the feature map is the result of the convolution of a kernel with a region in the input image, called the {\it local receptive field for the hidden neuron}, Fig.~\ref{fig:Convolutional_layers}. The local receptive field is a three dimensions window on the input voxels. This is then followed by a pooling operation which is a form of down-sampling of the newly produced feature map. Existing between the convolution and the pooling layer is an activation function such as the Rectified Linear unit ({\it ReLu}) layer; a non-saturating activation is applied element-wise, {\it i.e.} $f(x) = \max(0,x)$ thresholding at zero. After several convolutional and pooling layers, the image size (feature map size) is reduced and more complex features are extracted. \\

\begin{figure}[htbp]
   \begin{center}
      \includegraphics[scale=0.3, angle=0]{images/Bishop_cnn_layer.jpg}
   \end{center}
   \caption{Convolutional layers. Receptive field of a convolutional layer. Each neuron of the feature map is the result of the convolution of a kernel with the input image in the receptive field.}
  \label{fig:Convolutional_layers} 
\end{figure}

\begin{figure*}[htbp]
   \begin{center}
      \includegraphics[scale=1., angle=0]{images/1-s2_0-S0149763416305176-gr4.jpg}
   \end{center}
   \caption{Generic structure of a CNN. For illustrative purpose, this example only has one layer of each type; a real-world CNN, however, would have several convolutional and pooling layers (usually interpolated) and one fully-connected layer. (a) Input layer. In its simplest way, the data is inputted into the network in such a way that each pixel corresponds to one node in the input layer. (b) Convolutional layer. A $3 \times 3$ filter or kernel (in green) is used to multiply the spatially corresponding $3 \times 3$ nodes in the image. The resulting weighted sum is then passed through a nonlinear function to derive the output value of one node in the feature map. The repetition of this same operation across all possible receptive fields results in one complete feature map. The same procedure with different kernels (in orange and blue) will result in separate complete feature maps. (c) Pooling layer. The size of each feature map can be reduced by taking the maximum value (or average) from a receptive field in the previous layer. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)}
  \label{fig:features_maps} 
\end{figure*}

\paragraph{Feature map}{The weights sharing strategy means that all the neurons in the first hidden layer detect exactly the same feature at different locations in the input image. A feature detected by a hidden neuron as the input pattern that will cause the neuron to activate: it might be an edge in the image, for instance, or maybe some other type of shape. That ability is also likely to be useful at other places in the image. And so it is useful to apply the same feature detector everywhere in the image. To put it in slightly more abstract terms, convolutional networks are well adapted to the translation invariance of images: move a picture of a cat a little ways, and it's still an image of a cat. To increase the feature coverage we create multiple kernels and use the same procedure with different kernels (in orange and blue) will result in separate complete feature maps Fig.~\ref{fig:features_maps} (\ToDo{How to initial the weights between two feature maps?})}

\subsubsection{Convolution}

Given a three dimentional input image $Im$ and a filter (kernel) $Ker$ of dimensions $I \times J \times K$, the convolution operation is given by:

\begin{equation}
  \begin{split}
    (Im*Ker)_{xyz} = & \sum_{i=1,j=1,k=1}^{I \times J \times K} Im(x-i,y-j,z-k)Ker(i,j,k)\\
    = & \sum_{ijk}^{I \times J \times K} Im(x+i,y+j,z+k)Ker(-i,-j,-k)
  \end{split}
  \label{eq:convolution} 
\end{equation}

Eq.~(\ref{eq:convolution}) is represented on the Fig~\ref{fig:Kernel}. In the case of neuroimages, we could have $M$ modalities in the first CNN layer, or feature maps in the previouse CNN layer as inputs such that $Im \in \mathbb{R}^{X \times Y \times Z}$. Subsequently, for one convolutional layer with a bank of $S$ filters we have $Ker \in \mathbb{R}^{I \times J \times K \times S}$ and biases $b \in \mathbb{R}^{S}$, one for each filter. The feature map is for a modality $m$:

\begin{equation*}
    (Im*Ker)_{sxyz} = \sum_{i=1,j=1,k=1}^{I \times J \times K} Im_{s,x+i,y+j,z+k,m}Ker_{-s,-i,-j,-k,-m} - b_{s}
  \label{eq:convolution_tot} 
\end{equation*}

For a voxel in layer $l_{u}$:

\begin{equation}
    a_{xyz;l_{u}}^{(s)} =  \sum_{s'=1}^{S'}\sum_{i=-I/2,j=-J/2,k=-K/2}^{I/2 \times J/2 \times K/2}\omega_{ijk;l_{u}}^{(s)}z_{x_{s'}+i,y_{s'}+j,z_{s'}+k;l_{u-1}}^{(s')} + b_{l_{u}}^{(s)}
  \label{eq:convolution_tot_vox} 
\end{equation}

where $a_{xyz;l_{u}}^{(s)}$ is the convolved activation voxel in the feature map $(s)$ corresponding to the kernel $(s)$, in the layer $l_{u}$. $S$ is the set of kernels used in the layer $l_{u}$. $S'$ is the set of kernel used to build feature maps in the layer $l_{u-1}$. For the feature map $s$, $\{\omega^{(s)}, b^{(s)}\}$ represents the kernel $s$. And $s$ is one of the kernels used to build the feature maps at the level $l_{u}$. If the current layer is the first layer, $s$ represents the set of input images. Otherwise, $s$ is the set of feature maps created by the previouse convolutional layer. The position $(x',y',z')_{l_{u-1}}$ is the center of the receptive field window. In our developments the input map and the feature map have the same dimension: $(x',y',z')_{l_{u-1}} = (x,y,z)_{l_{u}}$. $z$ is the output at the level $l_{u-1}$ ($z_{xyz;l_{u}}^{(s)} = f(a_{xyz;l_{u}}^{(s)})$ where $f$ is an activation function). $s$ is the set layers built in the previous layer. All the feature maps of a previouse level $l_{u-1}$ are filtered by the weights of the current level to create the feature maps of the current leve $l_{u}$.



\begin{figure}[htbp]
   \begin{center}
      \includegraphics[scale=0.3, angle=0]{images/GvsBA.jpg}
   \end{center}
   \caption{Kernel. Two dimensions representation of the kernel action.}
  \label{fig:Kernel} 
\end{figure}



\subsection{Foward Propagation}

To perform a convolution operation, the kernel is flipped $180^{\circ}$ and slid across the input feature maps in equal and finite strides. At each location, the product between each element of the kernel and the input feature map element it overlaps is computed and the results summed up to obtain the output at that current location Eq.~(\ref{eq:convolution_tot_vox}). This procedure is repeated using different kernels to form as many output feature maps as desired Fig.~\ref{fig:features_maps}.

\subsection{Backpropagation}

For backpropagation there are two updates performed, for the weights and the deltas. Lets begin with the weight update. We are looking to compute $\partial E_{i} / \partial \omega_{uvw;l}^{(s)}$ which can be interpreted as the measurement of how the change in a single pixel $\omega_{uvw;l_{u}}^{(s)}$ in the weight kernel affects the loss function $E_{i}$. Convolution between the input feature maps of dimension $X \times Y \times Z$  and the weight kernel of dimension $I \times J \times K$ produces an output feature map of size $X \times Y \times Z$. The gradient component for the individual weights can be obtained by applying the chain rule Eq.~(\ref{eq:conv_backward_weights}).\\

%\lipsum[3-4]

\begin{equation}
  \begin{split}
    \frac{\partial E_{i}}{\partial \omega_{ijk;l_{u}}^{(s)}} =& \sum_{xyz}^{X \times Y \times Z} \frac{\partial E_{i}}{\partial a_{xyz;l_{u}}^{(s)}}\frac{\partial a_{xyz;l_{u}}^{(s)}}{\partial \omega_{ijk;l_{u}}^{(s)}}  \\
    =& \sum_{xyz}^{X \times Y \times Z} \delta_{l_{u}}^{(s)}(x,y,z) \, \sum_{s'}^{S'} z_{x'+i,y'+j,z'+k;l_{u-1}}^{(s')} \\
  \end{split}
  \label{eq:conv_backward_weights} 
\end{equation}

$\delta_{l_{u}}^{(s)}$ is the estimation of the error at the feature map $s$, at the position $(x,y,z)_{l_{u}}$ on the layer $l_{u}$. Using equation~(\ref{eq:convolution_tot_vox}) we can etimate the second part of the Eq.~(\ref{eq:conv_backward_weights}). For all activatios of the layer $l_{u}$, we have:

\begin{equation*}
  \begin{split}
    \left( \frac{\partial a_{xyz;l_{u}}}{\partial \omega_{ijk;l_{u}}} \right)^{(s)} =& \frac{\partial }{\partial \omega_{ijk;l_{u}}^{(s)}}\sum_{\sigma}\sum_{\alpha \beta \gamma} \omega_{\alpha \beta \gamma;l_{u}}^{(s)}z_{x'+\alpha,y'+\beta,z'+\gamma;l_{u-1}}^{(\sigma)}\\
    =& \sum_{\sigma}\sum_{\alpha \beta \gamma} \delta_{i \alpha} \delta_{j \beta} \delta_{k \gamma} \, z_{x'+\alpha,y'+\beta,z'+\gamma;l_{u-1}}^{(\sigma)}\\
    =& \sum_{\sigma} z_{x'+i,y'+j,z'+k;l_{u-1}}^{(\sigma)}
  \end{split}
\end{equation*}

Where $(x',y',z')$ is the center of the receptive field associated to $(x,y,z)$. In our case, the input map and the feature map have exactly the size, we should have a strict correspondance between $(x',y',z')_{l_{u-1}}$ and $(x,y,z)_{l_{u}}$. The first part of the equation, the error at the level $l_{u}$ of the feature map $s$, can be solved in the following way:

\lipsum[20-20]

\begin{strip}
\begin{equation}
  \begin{split}
    \frac{\partial E_{i}}{\partial a_{xyz;l_{u}}^{(s)}} =& \, \delta_{l_{u}}^{(s)}(x,y,z) \\ 
    =& \sum_{s''}^{S''}\sum_{x''y''z''}^{X \times Y \times Z} \frac{\partial E_{i}}{\partial a_{x''y''z'';l_{u+1}}^{(s'')}} \frac{\partial a_{x''y''z'';l_{u+1}}^{(s'')}}{\partial a_{xyz;l_{u}}^{(s)}} \\
    =& \sum_{s''}\sum_{x''y''z''} \delta_{l_{u+1}}^{(s'')}(x'',y'',z'') \frac{\partial a_{x''y''z'';l_{u+1}}^{(s'')}}{\partial a_{xyz;l_{u}}^{(s)}} \\
    =& \sum_{s''}\sum_{x''y''z''} \delta_{l_{u+1}}^{(s'')}(x'',y'',z'')  \frac{\partial }{\partial a_{xyz;l_{u}}^{(s)}} \sum_{\sigma}\sum_{\alpha \beta \gamma} \omega_{\alpha \beta \gamma;l_{u+1}}^{(s'')}z_{x''+\alpha,y''+\beta,z''+\gamma;l_{u}}^{(\sigma)} + b_{l_{u+1}}^{(s'')}\\
    =& \sum_{s''}\sum_{x''y''z''} \delta_{l_{u+1}}^{(s'')}(x'',y'',z'') \sum_{\sigma}\sum_{\alpha \beta \gamma} \omega_{ \alpha \beta \gamma;l_{u+1}}^{(s'')}  \frac{\partial }{\partial a_{xyz;l_{u}}^{(s)}} f(a_{x''+\alpha,y''+\beta,z''+\gamma;l_{u}}^{(\sigma)})\\
    =& \sum_{s''}\sum_{x''y''z''} \delta_{l_{u+1}}^{(s'')}(x'',y'',z'') \sum_{\sigma}\sum_{\alpha \beta \gamma} \omega_{ \alpha \beta \gamma;l_{u+1}}^{(s'')} \delta_{s \sigma} \delta(x-(x''+\alpha))  \delta(y-(y''+\beta))  \delta(z-(z''+\gamma))  f'(a_{x''+\alpha,y''+\beta,z''+\gamma;l_{u}}^{(\sigma)})\\
    =& f'(a_{x,y,z;l_{u}}^{(s)}) \sum_{s''}\sum_{\alpha \beta \gamma} \delta_{l_{u+1}}^{(s'')}(x-\alpha, y-\beta, z-\gamma) \, \omega_{\alpha \beta \gamma;l_{u+1}}^{(s'')}
  \end{split}
  \label{} 
\end{equation}
\end{strip}


\subsection{Up sampling}

\lipsum[21-21]

\subsection{Pooling Layer}

The function of the pooling layer is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. No learning takes place on the pooling layers. Pooling units are obtained using functions like max-pooling, average pooling and even L2-norm pooling. At the pooling layer, forward propagation results in an $N \times N$ pooling block being reduced to a single value -- value of the {\it winning unit}. Backpropagation of the pooling layer then computes the error which is acquired by this single value winning unit. To keep track of the winning unit its index noted during the forward pass and used for gradient routing during backpropagation. Gradient routing is done in the following ways:

\begin{itemize}
    \item Max-pooling - the error is just assigned to where it comes from - the “winning unit” because other units in the previous layer’s pooling blocks did not contribute to it hence all the other assigned values of zero
    \item Average pooling - the error is multiplied by $1 / (N \times N)$ and assigned to the whole pooling block (all units get this same value).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Auto-encoder}

%@inproceedings{Masci:2011:SCA:2029556.2029563,
% author = {Masci, Jonathan and Meier, Ueli and Cire\c{s}an, Dan and Schmidhuber, J\"{u}rgen},
% title = {Stacked Convolutional Auto-encoders for Hierarchical Feature Extraction},
% booktitle = {Proceedings of the 21th International Conference on Artificial Neural Networks - Volume Part I},
% series = {ICANN'11},
% year = {2011},
% isbn = {978-3-642-21734-0},
% location = {Espoo, Finland},
% pages = {52--59},
% numpages = {8},
% url = {http://dl.acm.org/citation.cfm?id=2029556.2029563},
% acmid = {2029563},
% publisher = {Springer-Verlag},
% address = {Berlin, Heidelberg},
% keywords = {auto-encoder, classification, convolutional neural network, unsupervised learning},
%} 
In this section we are reviewing the auto-encoder. The auto-encoder algorithm is well suited for segmentation, denoising and training convolutional layers of a more complex system. The later allows the extration of features and compet with acceleration algorithm in classification algorithms. This feature extration has been frequently used in computer vision and recognation. This section is inpired by~\cite{Masci:2011:SCA:2029556.2029563}.
An auto-encoder takes an input $x \in \mathbb{R}^{d}$ and first maps it to the latent representation $\bm{h} \in \mathbb{R}^{d}$ using a deterministic function of the type $\bm{h} = f_{\theta} = \sigma(Wx + b)$ with parameters $\theta = \{W, b\}$. This "code" is then used to reconstruct the input by a reverse mapping of $\bm{y} = f_{\theta'} = \sigma( W'h + b')$ with $\theta' = \{W', b'\}$. The two parameter sets are usually constrained to be of the form $W' = W^{T}$, using the same weights for encoding the input and decoding the latent representation. The parameters are optimized, minimizing an appropriate cost function over the training set.\\

\begin{eqnarray*}
  \left(
  \begin{array}{cccc}
    \omega_{00} & \omega_{01} & \cdots & \omega_{0(n+1)} \\
    \omega_{10} & \omega_{11} & \cdots & \omega_{1(n+1)} \\
    \vdots \\
    \omega_{m0} & \omega_{m1} & \cdots & \omega_{m(n+1)} \\
  \end{array}
  \right) \left(
  \begin{array}{c}
    x_{0} \\
    x_{1} \\
    \vdots \\
    x_{n} \\
    1 \\
  \end{array}
  \right) + b = \left(
  \begin{array}{c}
    a_{0} \\
    a_{1} \\
    \vdots \\
    a_{m} 
  \end{array}
  \right)
\end{eqnarray*}

Then the decoding process $W^{T}h + b'$:

\begin{eqnarray*}
  \left(
  \begin{array}{cccc}
    \omega_{00} & \omega_{10} & \cdots & \omega_{m0} \\
    \omega_{01} & \omega_{11} & \cdots & \omega_{m1} \\
    \vdots \\
    \omega_{0(n+1)} & \omega_{1(n+1)} & \cdots & \omega_{m(n+1)} \\
  \end{array}
  \right) \left(
  \begin{array}{c}
    z_{0} \\
    z_{1} \\
    \vdots \\
    z_{m} 
  \end{array}
  \right) + b' = \left(
  \begin{array}{c}
    y_{0} \\
    y_{1} \\
    \vdots \\
    y_{n} 
  \end{array}
  \right)
\end{eqnarray*}

And we apply the back propagation epoque after epoque until $x \sim y$.

\subsection{Denoising Auto-encoder}

Without any additional constraints, conventional auto-encoders learn the identity mapping. This problem can be circumvented by using a probabilistic RBM approach, or sparse coding, or denoising auto-encoders (DAs) trying to reconstruct noisy inputs. The latter performs as well as or even better than RBMs. Training involves the reconstruction of a clean input from a partially destroyed one. Input $x$ becomes corrupted input $x'$ by adding a variable amount $v$ of noise distributed according to the characteristics of the input image. Common choices include binomial noise (switching pixels on or off) for black and white images, or uncorrelated Gaussian noise for color images. The parameter $v$ represents the percentage of permissible corruption. The auto-encoder is trained to denoise the inputs by first finding the latent representation $\bm{h} = f_{\theta} = \sigma(Wx' + b)$ frome which to reconstruct the original input $\bm{y} = f_{\theta'} = \sigma( W'h + b')$.

\subsection{Stacked Convolutional Auto-Encoders (CAE)}

Convolutional auto-encoder differs from conventional auto-encoders as their weights are shared among all locations in the input, preserving spatial locality. The reconstruction is hence due to a linear combination of basic image patches based on the latent code. The CAE architecture is intuitively similar to the one described in the previouse section, except that the weights are shared. 

%%%\begin{equation*}
%%%h^{k} = \sigma(x*W^{k} + b^{k})
%%%\end{equation*}
%%%
%%%The reconstruction is obtained using
%%%
%%%\begin{equation*}
%%%y = \sigma \left(\sum_{k \in H} h^{k}*\tilde{W}^{k} + c \right)
%%%\end{equation*}
%%%
%%%where again there is one bias $c$ per input channel. $H$ identifies the group of latent feature maps; $\tilde{W}$ identifies the flip operation over both dimensions of the weights. The cost function to minimize is the mean squared error (MSE):
%%%
%%%\begin{equation*}
%%%E= \frac{1}{2n} \sum_{i=1}^{n} ( x_{i} - y_{i})^{2}
%%%\end{equation*}

\subsubsection{Mathematical description of the backpropagation}

An auto-encoder takes an input $z_{l_{u-1}}^{(s)} \in \mathbb{R}^{d}$ and first maps it to the latent representation $a_{l_{u}^{(\sigma)}} \in \mathbb{R}^{d}$ using a deterministic function of the type $z_{x,l_{u}}^{(\sigma)} = f_{\theta}(\sum_{s,\epsilon} \omega_{\epsilon,l_{u}}^{(\sigma)}z_{x+\epsilon,l_{u-1}}^{(s)} + b_{l_{u}}^{(\sigma)})$ with parameters $\theta = \{\omega_{l_{u}}, b_{l_{u}}^{(\sigma)}\}$. This "code" is then used to reconstruct the input by a reverse mapping of $z_{x,l_{u+1}}^{(s)} = f_{\theta'}(\sum_{\sigma,\epsilon} \omega_{\epsilon,l_{u}}^{(\sigma)}z_{x-\epsilon,l_{u}}^{(\sigma)} + b_{l_{u+1}}^{(s)})$ with $\theta_{l_{u+1}} = \{W', b_{l_{u+1}}^{(s)}\}$. The two parameter sets are usually constrained to be of the form $W' = W_{l_{u}}^{T}$, using the same weights for encoding the input and decoding the latent representation. The parameters are optimized, minimizing an appropriate cost function over the training set. In all the equations $(s)$ represents the number of inputs and $(\sigma)$ the number of feature maps created in the process. In the auto-encoder, the number of inputs and final outputs can match. In other uses, {\it e.g.} the segmentation, the number of input can be the different acquisition types and the number of outputs can different segmentations.\\
The encoding level is similar to a convolutional layer: in a kernel window of $\epsilon$ elements, activations $a_{x,l_{u}}^{\mu}$ at the position $x$ in the layer $l_{u}$ 

\begin{eqnarray*}
  \left(
  \begin{array}{cccc}
    \omega_{0}^{(1)} & \omega_{1}^{(1)} & \cdots & \omega_{(\nu)}^{(1)} \\
    \omega_{0}^{(2)} & \omega_{1}^{(2)} & \cdots & \omega_{(\nu)}^{(2)} \\
    \vdots \\
    \omega_{0}^{(\sigma)} & \omega_{1}^{(\sigma)} & \cdots & \omega_{(\nu)}^{(\sigma)} \\
  \end{array}
  \right)_{l_{u}} \left(
  \begin{array}{c}
    z_{0} \\
    z_{1} \\
    \vdots \\
    z_{\nu-1} \\
    1 \\
  \end{array}
  \right)_{l_{u-1}}^{(1,2,\cdots,s)} + b_{l_{u}}^{(\sigma)}= \left(
  \begin{array}{c}
    a^{(1)} \\
    a^{(2)} \\
    \vdots \\
    a^{(\sigma)} 
  \end{array}
  \right)_{x,l_{u}}
\end{eqnarray*}

Wich can be written: $z_{x,l_{u}}^{(\sigma)} = f(a_{x,l_{u}}^{(\sigma)})$ where $a_{x,l_{u}}^{(\sigma)} = \sum_{\epsilon,s} \omega_{\epsilon,l_{u}}^{(\sigma)}z_{x+\epsilon,l_{u-1}}^{(s)} + b_{l_{u}}^{(\sigma)}$. In a similar way, as the auto-encoder, we can use the same set of weights for the convolutional auto-encoder, then we can write


\begin{strip}
  \begin{eqnarray*}
      \left(
      \begin{array}{cccc}
        \omega_{0}^{(1)} & \omega_{0}^{(2)} & \cdots & \omega_{0}^{(\sigma)} \\
        \omega_{1}^{(1)} & \omega_{1}^{(2)} & \cdots & \omega_{1}^{(\sigma)} \\
        \vdots \\
        \omega_{\nu-1}^{(1)} & \omega_{\nu-1}^{(2)} & \cdots & \omega_{\nu-1}^{(\sigma)} \\
      \end{array}
      \right) \left(
      \begin{array}{c}
        z_{0}^{(1)} \\
        z_{0}^{(2)} \\
        \vdots \\
        z_{0}^{(\sigma)} \\
      \end{array}
      \right) + \left. 
      \begin{array}{c}
        z_{1}^{(1)} \\
        z_{1}^{(2)} \\
        \vdots \\
        z_{1}^{(\sigma)} \\
      \end{array}
      \right) + \cdots + \left. 
      \begin{array}{c}
        z_{\nu-1}^{(1)} \\
        z_{\nu-1}^{(2)} \\
        \vdots \\
        z_{\nu-1}^{(\sigma)} \\
      \end{array}
      \right)_{x,l_{u}} + b_{l_{u+1}}^{(s)};~and~f_{\theta}\left(\sum_{\sigma} z_{l_{u}}^{(\sigma)}*\tilde{W}^{(\sigma)} + b_{l_{u+1}}^{(s)} \right) = y_{x,l_{u+1}}^{(s)}
  \end{eqnarray*}
\end{strip}

This can be written in a more compact way, $z_{x,l_{u+1}}^{(\nu)} = f(a_{x,l_{u+1}}^{(\nu)})$ where $(\nu)$ can be equal to $(s)$ in the auto-encoder algorithm. And $a_{x,l_{u+1}}^{(\nu)} = \sum_{\epsilon,\sigma} \omega_{\epsilon,l_{u}}^{(\sigma)}z_{x+\epsilon,l_{u}}^{(\sigma)} + b_{l_{u+1}}^{(\nu)}$. \\
We have two levels of the auto-encoder sharing the same weights. We can use the backpropagation through the two levels. In this example, the loss function is the sum of squares $E_{i} = \sum_{s} (z_{l_{u-1}}^{(s)} - z_{l_{u+1}}^{(s)})^{2}$. As usual, we start with:

\begin{eqnarray}
  \frac{\partial E_{i}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} & = & \sum_{\nu,x} \frac{\partial E_{i}}{\partial a_{x,l_{u+1}}^{(\nu)}} \frac{a_{x,l_{u+1}}^{(\nu)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} + \sum_{\sigma,x} \frac{\partial E_{i}}{\partial a_{x,l_{u}}^{(\sigma)}} \frac{a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} \\
 & = & \sum_{\nu,x} \delta_{l_{u+1}}^{(\nu)}(x) \frac{a_{x,l_{u+1}}^{(\nu)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} + \sum_{\sigma,x} \delta_{l_{u}}^{(\sigma)}(x) \frac{a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} \\
 & = & \sum_{\nu} \delta_{l_{u+1}}^{(\nu)} \underset{\epsilon'}{*} z_{l_{u}}^{(\sigma')}  + \sum_{s} \delta_{l_{u}}^{(\sigma')} \underset{\epsilon'}{*} z_{l_{u-1}}^{(s)}  \\
\end{eqnarray}
\label{equa:AE_1}

Where $\underset{\epsilon'}{*}$ represents the center $\epsilon'$ of the window convolved across the entire image. The last equation can be found using the following evaluation:

\begin{eqnarray*}
  \left \lbrace
  \begin{array}{rcl}
    \frac{a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} & = & \sum_{s,\epsilon} \delta_{\epsilon' \epsilon} \delta_{\sigma' \sigma} z_{x+\epsilon,l_{u-1}}^{(s)} = \sum_{s} \delta_{\sigma' \sigma} z_{x+\epsilon',l_{u-1}}^{(s)}\\
    \delta_{l_{u}}^{(\sigma)}(x) & = & \sum_{\nu,x'} \frac{\partial E_{i}}{\partial a_{x',l_{u+1}}^{(\nu)}}\frac{\partial a_{x',l_{u+1}}^{(\nu)}}{\partial a_{x,l_{u}}^{(\sigma)}} = \sum_{\nu,x'} \delta_{l_{u+1}}^{(\nu)}(x')\frac{\partial a_{x',l_{u+1}}^{(\nu)}}{\partial a_{x,l_{u}}^{(\sigma)}} \\
    & = & \sum_{\nu,x'} \delta_{l_{u+1}}^{(\nu)}(x') \sum_{\sigma'',\epsilon} \omega_{\epsilon,l_{u}}^{(\sigma'')} (f')_{x'+\epsilon,l_{u}}^{(\sigma'')} \\
    & \times & \delta_{\sigma'' \sigma} \delta(x - (x'-\epsilon)) \\
    & = & \sum_{\nu,\epsilon} \delta_{l_{u+1}}^{(\nu)}(x-\epsilon) \omega_{\epsilon,l_{u}}^{(\sigma)} (f')_{x,l_{u}}^{(\sigma)}
  \end{array}
  \right .
\end{eqnarray*}

The second level can be evaluated:

\begin{eqnarray*}
  \left \lbrace
  \begin{array}{rcl}
    \frac{a_{x,l_{u+1}}^{(\nu)}}{\partial \omega_{\epsilon',l_{u}}^{(\sigma')}} & = & \sum_{\sigma,\epsilon} \delta_{\epsilon' \epsilon} \delta_{\sigma' \sigma} z_{x+\epsilon,l_{u}}^{(\sigma)} = z_{x+\epsilon',l_{u}}^{(\sigma')} \\
    \delta_{l_{u+1}}^{(\nu)}(x) & = & \left( z_{l_{u-1}}^{(\nu)} - z_{l_{u+1}}^{(\nu)} \right) f'(a_{x,l_{u+1}}^{(\nu)})
  \end{array}
  \right .
\end{eqnarray*}
  
\subsubsection{Mathematical description}

The activation of the encoding and deconding at the position $x$ can be written as, 1) encoding: $a_{x,l_{u}}^{(\sigma)} = \sum_{i,\epsilon'} \omega_{\epsilon'}^{(\sigma)} z_{x+\epsilon',l_{u-1}}^{(i)} + b^{(\sigma)} = \sum_{i} w^{(\sigma)} * z_{x,l_{u-1}}^{(i)}$ for the layer $l_{u}$ taking the activation functions $z_{x+\epsilon',l_{u-1}}$ of the previouse layer as inputs and using the set of parameters $w^{(\sigma)} = \lbrace \omega_{\epsilon'}^{(\sigma)}, b^{(\sigma}\rbrace$. The resulting activation function is $z_{x+\epsilon',l_{u}}^{(\sigma)} = f(a_{x+\epsilon',l_{u}}^{(\sigma)})$ ; 2) decoding: $a_{x,l_{u+1}}^{(o)} = \sum_{\sigma,\epsilon'} \omega_{\epsilon'}^{(\sigma)} z_{x+\epsilon',l_{u}}^{(\sigma)} + b^{(o)}$ for the layer $l_{u+1}$ taking the activation functions $z_{x+\epsilon',l_{u}}$ of the previouse layer as inputs and using the same weights parameters $w^{(o)} = \lbrace \omega_{\epsilon'}^{(\sigma)}, b^{(o)}\rbrace$ as the previous layer $l_{u}$. Only the bias $b^{(o)}$ dissociate the newly built activations from the others. The decoding activation can be written as a dot product of the convolutions: $a_{x,l_{u+1}}^{(o)} = \sum_{\sigma,\epsilon'} \omega_{\epsilon'}^{(\sigma)} z_{x+\epsilon',l_{u}}^{(\sigma)} + b^{(o)} = w^{T}*z_{x,l_{u}}$. Using the following total differential forms of the activations:

\begin{itemize}
  \item $da_{x,l_{u}}^{(\sigma)} = \sum_{\nu,\eta} \frac{\partial a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\eta}^{(\nu)}} d\omega_{\eta}^{(\nu)}$
  \item $da_{x,l_{u+1}}^{(o)} = \sum_{\nu,\eta} \frac{\partial a_{x,l_{u+1}}^{(o)}}{\partial \omega_{\eta}^{(\nu)}} d\omega_{\eta}^{(\nu)}$
\end{itemize}

Implementing the total differential form of activations in the eq.~(\ref{eq:total_diff}), we can write:

\begin{equation*}  
\begin{split}
  dE = & \sum_{o,x} \frac{\partial E}{\partial a_{x,l_{u+1}}^{(o)}} \sum_{\nu,\eta} \frac{\partial a_{x,l_{u+1}}^{(o)}}{\partial \omega_{\eta}^{(\nu)}} d\omega_{\eta}^{(\nu)} \\
     + & \sum_{\sigma,x} \frac{\partial E}{\partial a_{x,l_{l_{u}}}^{(\sigma)}} \sum_{\nu,\eta} \frac{\partial a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\eta}^{(\nu)}} d\omega_{\eta}^{(\nu)} \\
     = & \vec{\nabla }E \cdot \vec{dl}
\end{split}
\end{equation*}

To reach any partial derivative of the cost function at the element $\omega_{\epsilon}^{(\sigma')}$ of the gradient, for any $\epsilon$ and $\sigma'$, we solve:

\begin{equation*}  
\begin{split}
  \frac{\partial E}{\partial \omega_{\epsilon}^{(\sigma')}} = & \sum_{o,x} \delta_{l_{u+1}}^{(o)}(x)\sum_{\nu,\eta} \frac{\partial a_{x,l_{u+1}}^{(o)}}{\partial \omega_{\eta}^{(\nu)}} \delta_{\eta \epsilon'}\delta_{\nu \sigma'} \\
  + & \sum_{\sigma,x}  \delta_{l_{u}}^{(\sigma)}(x) \sum_{\nu,\eta} \frac{\partial a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\eta}^{(\nu)}} \delta_{\eta \epsilon'}\delta_{\nu \sigma'} \\
  = & \sum_{o,x} \delta_{l_{u+1}}^{(o)}(x) \frac{\partial a_{x,l_{u+1}}^{(o)}}{\partial \omega_{\epsilon}^{(\sigma')}}  + \sum_{\sigma,x} \delta_{l_{u}}^{(\sigma)}(x) \frac{\partial a_{x,l_{u}}^{(\sigma)}}{\partial \omega_{\epsilon}^{(\sigma')}}
\end{split}
\end{equation*}

Where the local errors for the layers $l_{u+1}$ and $l_{u}$ are $\frac{\partial E}{\partial a_{x,l_{u+1}}^{(o)}} = \delta_{l_{u+1}}^{(o)}(x)$ and $\frac{\partial E}{\partial a_{x,l_{l_{u}}}^{(\sigma)}} = \delta_{l_{u}}^{(\sigma)}(x)$. Taking the definition of the activation at the level $l_{u+1}$, the partial derivative over the weight $\partial a_{x,l_{u+1}}^{(o)}/\partial \omega_{\epsilon}^{(\sigma')} = \sum_{\sigma, \epsilon'} \delta_{\sigma \sigma'}\delta_{\epsilon \epsilon'} \left( z_{x+\epsilon', l_{u}}^{(\sigma)} + \omega_{\epsilon'}^{(\sigma)} \, \partial z_{x+\epsilon',l_{u}}^{(\sigma)} / \partial \omega_{\epsilon}^{(\sigma')}\right) = \sum_{\sigma, \epsilon'} \delta_{\sigma \sigma'}\delta_{\epsilon \epsilon'} \tilde{z}_{\epsilon'}^{\sigma}$. The same operation is made at the level $l_{u}$: $\partial a_{x,l_{u}}^{(\sigma)}/\partial \omega_{\epsilon}^{(\sigma')} = \delta_{\sigma \sigma'} \sum_{i, \epsilon'} \delta_{\epsilon \epsilon'} z_{x+\epsilon', l_{u-1}}^{(i)}$. the partial differential of the lost function, $E$, at the local weight $\omega_{\epsilon}^{(\sigma')}$ become: 

\begin{equation*}  
\begin{split}
  \frac{\partial E}{\partial \omega_{\epsilon}^{(\sigma')}} = & \sum_{o,x} \delta_{l_{u+1}}^{(o)}(x)  \left( z_{x+\epsilon, l_{u}}^{(\sigma')} + \omega_{\epsilon}^{(\sigma')} \, \frac{\partial z_{x+\epsilon,l_{u}}^{(\sigma')}}{\partial \omega_{\epsilon}^{(\sigma')} } \right) \\
  + & \sum_{x} \left( \delta_{l_{u}}^{(\sigma')}(x)  \sum_{i}  z_{x+\epsilon, l_{u-1}}^{(i)} \right) \\
  = & \sum_{o} \delta^{(o)} \underset{x}{*} \tilde{z}_{\epsilon}^{(\sigma')} + \delta^{\sigma'} \underset{x}{*} \sum_{i} z_{\epsilon}^{(i)}
\end{split}
\end{equation*}

Need some word how to treat $\tilde{z}$. \lipsum[16-16] 
We can evaluate the value of the loss error at the level $l_{u}$:

\begin{equation*}  
\begin{split}
  \delta_{l_{u}}^{(\sigma')}(x) = & \frac{\partial E}{\partial a_{x,l_{u}}^{(\sigma')}} = \sum_{o,X} \frac{\partial E}{\partial a_{X,l_{u+1}}^{(o)}} \frac{\partial a_{X,l_{u+1}}^{(o)}}{\partial a_{x,l_{u}}^{(\sigma')}} \\
  = & \sum_{o,X} \delta_{l_{u+1}}^{(o)}(X) \sum_{\sigma,\epsilon'} \omega_{\epsilon'}^{(\sigma)} \frac{\partial z_{X + \epsilon', l_{u}}^{(\sigma)}}{\partial a_{x,l_{u}}^{(\sigma')}} \delta_{\sigma \sigma'}\delta[x-(X+\epsilon')] \\
  = & \sum_{o,X} \delta_{l_{u+1}}^{(o)}(X) \sum_{\epsilon'} \omega_{\epsilon'}^{(\sigma')} \frac{\partial z_{X + \epsilon', l_{u}}^{(\sigma')}}{\partial a_{x,l_{u}}^{(\sigma')}} \delta[x-(X+\epsilon')] \\
  = & f'(a_{x,l_{u}}^{(\sigma')}) \sum_{o,\epsilon'} \delta_{l_{u+1}}^{(o)}(x-\epsilon') \, \omega_{\epsilon'}^{(\sigma')}  \\
\end{split}
\end{equation*}

the loss function become:

\begin{eqnarray*}  
\begin{array}{rl}
  \frac{\partial E}{\partial \omega_{\epsilon}^{(\sigma')}} = \sum_{o,x} & \left\{ \delta_{l_{u+1}}^{(o)}(x) \tilde{z}_{x+\epsilon, l_{u}}^{(\sigma')} \right. \\
  & +  \left. f'(a_{x,l_{u}}^{(\sigma')}) \sum_{\epsilon'} \delta_{l_{u+1}}^{(o)}(x-\epsilon') \, \omega_{\epsilon'}^{(\sigma')} \sum_{i}  z_{x+\epsilon, l_{u-1}}^{(i)} \right\}
\end{array}
\end{eqnarray*}



 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Densely connected neural network}

Dense neural networks are caracterized by a full connection of a neurons of one layer with all neurons plus the bias from the previouse layer Fig.~\ref{fig:Densely_connected_neural_network} presents the activation function for each layer. Each neuron, or activation, of a layer is the result of a perceptron, built from all the nerons of the previous layer and the bias, and filtered by an activation function. 


\begin{figure}[htbp]
   \begin{center}
      \includegraphics[scale=0.3, angle=0]{images/densely_connected_nn.png}
   \end{center}
   \caption{Densely connected neural network. The red dots represnet the input, the green dots represent the hidden layers, the blue dots represent the output layer. the black dots are the bias. The output layer is the only layer without a bias weight.}
  \label{fig:Densely_connected_neural_network} 
\end{figure}


  There are several choises for the activation function and we will try to provide the possibility of using several of them. However, the first developpments will be done with the hyperbolic tangent for the activation of the inside layers neurons: $f = \tanh$ and $f' = (1 - \tanh^{2})$. The last layer, the output layer, will be calculated with a soft maximum: $g(z_{l_{k}}) = e^{a_{l_{k}}} / \mathcal{Z}$, where the partition function $\mathcal{Z} = \sum_{l_{k} = 1}^{L_{k}} e^{z_{l_{k}}}$, and $g' = g(1 - g)$.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Backward propagation}

To solve the problem of the backward propagation, we have three options. The first option is to estimate the error for the full training set, which is heavy for the algorithm. The second option is the gradient descent in its stochastic version: each iteration will use a new intput, instead of estimating the cost function with the entire input population. The third option is a mid-point between the two first options: the error is estimated with a sub set of the full training set.
Taking the cross-enropy cost function, eq.~(\ref{cross_entropy}), in the case of classification. The gradient descent method used to find the minimum of the cost function is written:

\begin{equation}
  \bm{\omega}^{e+1} = \bm{\omega}^{e} - \eta \bm{\nabla} E
  \label{gradient_descent}
\end{equation}

Where $\bm{\omega}$ represents the vector of weights, $e$ represents the {\it epoque} (iteration), and $\eta$ represents the learning rate. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Algorithm}

The backward propagation for each layer, except the last layer, is a recursive estimtion of the deeper layer. The gradient Eq.~(\ref{gradient_descent}) at a particular weight is, using the error Eq.~(\ref{cost_function_error}),


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Verification and Validation}
\lipsum[20-20]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vanishing gradient}

The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs). As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks. Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[21] to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations.\\
\lipsum[19-19]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Early stoping}
\lipsum[21-21]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Drop out}
\lipsum[22-22]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monte-Rosa}

\lipsum[30-33]

Eventually with a small enough feature map, the contents are squashed into a one dimension vector and fed into a fully-connected MLP for processing. The last layer of this fully-connected MLP seen as the output, is a loss layer which is used to specify how the network training penalizes the deviation between the predicted and true labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mont-Blanc}

\lipsum[40-43]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\lipsum[6-10]

\section*{References}
%% References with bibTeX database:
\bibliographystyle{Bibliography/elsarticle-num}

\bibliography{Bibliography/sample}


\end{document}
