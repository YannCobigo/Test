%% sudo yum install tetex
%% sudo yum install texlive-elsarticle.noarch
%% pdflatex neural_network.tex && bibtex neural_network.aux && pdflatex neural_network.tex && pdflatex neural_network.tex


%\documentclass[a4paper,12pt]{}
\documentclass[final, paper=letter,5p,times,twocolumn]{elsarticle}
%\documentclass[preprint,review,8pt,times]{elsarticle}


%% or use the graphicx package for more complicated commands
%\usepackage{changebar}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amsthm,multicol,bm} % Math packages
%\usepackage{dsfont} % mathds{1}
%\usepackage{widetext} % 
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
%
%\usepackage[]{algorithm2e}
%% Macro
\newcommand{\ToDo}[1]{ToDo: \textbf{\textit{#1}}}
\newcommand{\CA}{computational anatomy}
%
\newdefinition{definition}{Definition}%
\newtheorem{theorem}{Theorem}%
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
%\newproposition{proposition}{Proposition}%
%\newlemma{lemma}{Lemma}%
%\AtEndEnvironment{theorem}{\null\hfill\qedsymbol}%

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frontmatter}

\title{Neural network}

\author[label1]{Yann Cobigo\corref{cor1}}
\address[label1]{University of California, San Francisco | ucsf.edu}
%\address[label2]{Address Two\fnref{label4}}

%\cortext[cor1]{I am corresponding author}
%\fntext[label3]{I also want to inform about\ldots}
%\fntext[label4]{Small city}

\ead{yann.cobigo@ucsf.edu}
\ead[url]{https://github.com/YannCobigo}

%% \author[label5]{Author Two}
%% \address[label5]{Some University}
%% \ead{author.two@mail.com}
%% 
%% \author[label1,label5]{Author Three}
%% \ead{author.three@mail.com}

\begin{abstract}
In this report we will \dots
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Fijee \sep electrode \sep PEM \sep CEM
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}


\ToDo{Def. geodisic} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Densly connected neural network}
\subsection{Description and notation}

Dense neural network are cracterized by a full connection of a neuron of one layer with all the neuron from the previouse layer. Table~\ref{Layers_activations} presents the activation function for each layer. In this document, the convention is $l = l_{0}$ for the input layer: $z_{l_{0} = 1} = x_{1}$, $z_{l_{0} = 2} = x_{2}$, \dots. The convention is to save the index 0 for the bias: $z_{l_{0} = 0} = x_{0} = b_{0}$ the bias on the input level. The last level is the output level: $l = l_{k}$, $z_{l_{k}} = y_{k}$. The output level doest not have a bias node.

\begin{table}[]
\centering
\caption{Neuron activation for each layers.}
\label{Layers_activations}
\begin{tabular}{llllll}
 $\{ z_{l_{0}}\}_{l_{0} = 0}^{L_{0}}$&  $\{ z_{l_{1}}\}_{l_{1} = 0}^{L_{1}}$ &  $\cdots$ & $\{ z_{l_{k-1}}\}_{l_{k-1} = 0}^{L_{k-1}}$ &  $\{ z_{l_{k}}\}_{l_{k} = 1}^{L_{k}}$ &  \\ 
\end{tabular}
\end{table}

Each neuron is an activated function of a linear combinaison of the neurons from the previous layer.

\begin{itemize}
    \item [$l = l_{0}$] we are at the level of the inputs
    \item [$l = l_{1}$] $a_{l_{1}} = \sum_{l_{0} = 0}^{L_{0}} \omega_{l_{1}l_{0}} z_{l_{0}} = \omega_{l_{1}}^{T} z^{(0)}$. Activation: $z_{l_{1}} = f(a_{l_{1}})$
    \item [$\vdots$]
    \item [$l = l_{k-1}$] $a_{l_{k-1}} = \sum_{l_{k-2} = 0}^{L_{k-2}} \omega_{l_{k-1}l_{k-2}} z_{l_{k-2}} = \omega_{l_{k-1}}^{T} z^{(k-2)}$. Activation: $z_{l_{k-1}} = f(a_{l_{k-1}})$
    \item [$l = l_{k}$] $a_{l_{k}} = \sum_{l_{k-1} = 0}^{L_{k-1}} \omega_{l_{k}l_{k-1}} z_{l_{k-1}} = \omega_{l_{k}}^{T} z^{(k-1)}$. Activation: $z_{l_{k}} = g(a_{l_{k}})$
\end{itemize}

In this enumeration $z^{(i)}$ is the vector of all neuron on the layer $i$. There are several choises for the activation function and will try to provide the possibility of using several of them. However, the first developpments will be done with the hyperbolic tangent for the activation of the inside layers nerons: $f = \tanh$ and $f' = (1 - \tanh^{2})$. The last layer, the output layer, will be calculated with a soft maximum: $g(z_{l_{k}}) = e^{z_{l_{k}}} / \mathcal{Z}$, where $\mathcal{Z} = \sum_{l_{k} = 1}^{L_{k}} e^{z_{l_{k}}}$, and $g' = g(1 - g)$.
  
\subsection{Forward propagation}

The forward propagation is straight forward. For a solution, $y_{k} = z_{l_{k}} = g(a_{l_{k}})$ where
$$
a_{l_{k}} = \omega_{l_{k}}^{T} z^{(k-1)} = \omega_{l_{k}}^{T} f(\omega_{l_{k-1}}^{T} z^{(k-2)}) = \omega_{l_{k}}^{T} f(\omega_{l_{k-1}}^{T} f(\omega_{l_{k-2}}^{T} z^{(k-3)})) = \dots
$$

\subsubsection{Algorithm}

The Table~\ref{weights_distribution} guives the representation of the weights in a dense neural network. Those weights can be represented in one long array in the hardware memory Table~\ref{weights_in_mem}.

\begin{table}[]
\centering
\caption{Weight distribution per layer.}
\label{weights_distribution}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$l_{0}$                   && 0                                   & 1                        & $\cdots$ & $L_{0}$ \\ \hline
\multirow{4}{*}{$l_{1}$}  &0& $\omega_{l_{1}=0l_{0}=0}$              & $\omega_{l_{1}=0l_{0}=1}$    &        & $\omega_{l_{1}=0l_{0}=L_{0}}$ \\ \cline{2-5} 
                         &1& $\omega_{l_{1}=1l_{0}=0}$              & $\omega_{l_{1}=1l_{0}=1}$    &        & $\omega_{l_{1}=0l_{0}=L_{0}}$ \\ \cline{2-5} 
                         &&                                     &                          & \vdots & \\ \cline{2-5} 
                         &$L_{1}$& $\omega_{l_{1}=L_{1}l_{0}=0}$      & $\omega_{l_{1}=L_{1}l_{0}=1}$ &        & $\omega_{l_{1}=L_{1}l_{0}=L_{0}}$ \\ \hline
$l_{1}$                   && 0                                   & 1                       &        & $L_{1}$ \\ \hline
\multirow{4}{*}{$l_{2}$}  &0& $\omega_{l_{2}=0l_{1}=0}$              &  $\omega_{l_{2}=0l_{1}=1}$   &        &  $\omega_{l_{2}=0l_{1}=L_{1}}$ \\ \cline{2-5} 
                         &1& $\omega_{l_{2}=1l_{1}=0}$              &  $\omega_{l_{2}=1l_{1}=1}$   &        &   $\omega_{l_{2}=0l_{1}=L_{1}}$ \\ \cline{2-5} 
                         &&                                     &                          & \vdots & \\ \cline{2-5} 
                         &$L_{2}$&$\omega_{l_{2}=L_{2}l_{1}=0}$       & $\omega_{l_{2}=L_{2}l_{1}=1}$ &        & $\omega_{l_{2}=L_{2}l_{1}=L_{1}}$ \\ \hline
\vdots                   &&                                     &                         &        & \\ \hline
$l_{k-1}$                 && 0                                   &  1                      &        &  $L_{k-1}$ \\ \hline
\multirow{4}{*}{$l_{k}$}  &1& $\omega_{l_{k}=1l_{k-1}=0}$            & $\omega_{l_{k}=1l_{k-1}=1}$  &        & $\omega_{l_{k}=1l_{k-1}=L_{k-1}}$ \\ \cline{2-5} 
                         &2& $\omega_{l_{k}=2l_{k-1}=0}$            & $\omega_{l_{k}=2l_{k-1}=1}$  &        &  $\omega_{l_{k}=2l_{k-1}=L_{k-1}}$ \\ \cline{2-5} 
                         &&                                    &                         & \vdots & \\ \cline{2-5} 
                         &$L_{k}$& $\omega_{l_{k}=L_{k1}l_{k-1}=0}$   & $\omega_{l_{k}=L_{k}l_{k-1}=1}$ &        &  $\omega_{l_{k}=L_{k}l_{k-1}=L_{k-1}}$ \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{My caption}
\label{weights_in_mem}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{$l_{1}$} & $\hdots$ & \multicolumn{3}{c|}{$l_{k}$} \\ \hline
$\omega_{l_{1}=0l_{0}=0}$   &   $\omega_{01}$   & $\hdots$  &  $\omega_{L_{1}L_{0}}$   & $\hdots$ &    $\omega_{l_{k}=1l_{k-1}=0}$    & $\hdots$  &   $\omega_{L_{k}L_{k-1}}$ \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{bal bal}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{blablabla}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{sect}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{sect}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{sect}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\section*{References}
%% References with bibTeX database:
\bibliographystyle{Bibliography/elsarticle-num}

\bibliography{Bibliography/sample}


\end{document}
